{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "865b03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f56c891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ded5a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc39c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8291b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8ce1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d84c54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77a67d6f2370>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
    "max_iters = 20000 #increase from 25000\n",
    "warmup_steps = 1000 #smoother initial train, earlier 100\n",
    "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
    "eval_iters = 500 # increased from 100\n",
    "batch_size = 32 # changed from 16, better gradient estimate\n",
    "block_size = 128 #changed from 64, capture longer range dependencies\n",
    "\n",
    "gradient_accumulation_steps = 32 # reduced from 50\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e633f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5611/2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a52254e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 500/20000 [00:53<33:51,  9.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 9.4174, val loss 9.4261\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1000/20000 [02:45<29:27, 10.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 8.4870, val loss 8.4928\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1499/20000 [04:41<36:00,  8.56it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500: train loss 7.5308, val loss 7.5339\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2000/20000 [06:35<27:53, 10.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: train loss 6.6740, val loss 6.6757\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2499/20000 [08:00<27:22, 10.65it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500: train loss 5.9826, val loss 5.9841\n",
      "The current learning rate: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2999/20000 [09:30<26:24, 10.73it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000: train loss 5.4799, val loss 5.4728\n",
      "The current learning rate: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3499/20000 [10:55<25:12, 10.91it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3500: train loss 5.0671, val loss 5.0584\n",
      "The current learning rate: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 3999/20000 [12:25<24:23, 10.93it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000: train loss 4.7442, val loss 4.7447\n",
      "The current learning rate: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4499/20000 [13:55<24:43, 10.45it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4500: train loss 4.5071, val loss 4.5018\n",
      "The current learning rate: 0.00013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5000/20000 [15:20<22:56, 10.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000: train loss 4.2880, val loss 4.2917\n",
      "The current learning rate: 0.00014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 5499/20000 [16:46<22:09, 10.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5500: train loss 4.1197, val loss 4.1152\n",
      "The current learning rate: 0.00015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 5999/20000 [18:16<21:14, 10.98it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000: train loss 3.9624, val loss 3.9642\n",
      "The current learning rate: 0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6499/20000 [19:40<21:14, 10.59it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6500: train loss 3.8161, val loss 3.8167\n",
      "The current learning rate: 0.00018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 6999/20000 [21:06<19:35, 11.06it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000: train loss 3.7047, val loss 3.7022\n",
      "The current learning rate: 0.00019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7499/20000 [22:30<19:11, 10.86it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7500: train loss 3.5953, val loss 3.5982\n",
      "The current learning rate: 0.00020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 7999/20000 [23:56<18:09, 11.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8000: train loss 3.5010, val loss 3.5045\n",
      "The current learning rate: 0.00022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8499/20000 [25:20<18:50, 10.17it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8500: train loss 3.4082, val loss 3.4077\n",
      "The current learning rate: 0.00024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9000/20000 [26:50<16:29, 11.11it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000: train loss 3.3191, val loss 3.3234\n",
      "The current learning rate: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 9500/20000 [28:16<16:43, 10.46it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9500: train loss 3.2524, val loss 3.2520\n",
      "The current learning rate: 0.00027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 9999/20000 [29:40<15:43, 10.60it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000: train loss 3.1785, val loss 3.1817\n",
      "The current learning rate: 0.00028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 10500/20000 [31:10<14:26, 10.96it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10500: train loss 3.1211, val loss 3.1190\n",
      "The current learning rate: 0.00030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 10999/20000 [32:36<14:13, 10.55it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11000: train loss 3.0465, val loss 3.0595\n",
      "The current learning rate: 0.00032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 11499/20000 [34:06<12:48, 11.06it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11500: train loss 2.9926, val loss 3.0036\n",
      "The current learning rate: 0.00033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 11999/20000 [35:30<11:59, 11.12it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12000: train loss 2.9383, val loss 2.9485\n",
      "The current learning rate: 0.00035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 12499/20000 [36:56<12:03, 10.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12500: train loss 2.8934, val loss 2.9016\n",
      "The current learning rate: 0.00036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13000/20000 [38:26<10:46, 10.82it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13000: train loss 2.8437, val loss 2.8505\n",
      "The current learning rate: 0.00038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13500/20000 [40:00<10:50, 10.00it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13500: train loss 2.8064, val loss 2.8031\n",
      "The current learning rate: 0.00040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14000/20000 [41:30<09:12, 10.86it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14000: train loss 2.7539, val loss 2.7562\n",
      "The current learning rate: 0.00041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 14499/20000 [43:06<08:45, 10.46it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14500: train loss 2.7152, val loss 2.7177\n",
      "The current learning rate: 0.00042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15000/20000 [44:30<07:46, 10.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15000: train loss 2.6842, val loss 2.6790\n",
      "The current learning rate: 0.00044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 15499/20000 [46:00<07:06, 10.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15500: train loss 2.6321, val loss 2.6460\n",
      "The current learning rate: 0.00045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 15999/20000 [47:26<06:06, 10.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16000: train loss 2.6004, val loss 2.6000\n",
      "The current learning rate: 0.00046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 16500/20000 [48:56<05:30, 10.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16500: train loss 2.5720, val loss 2.5779\n",
      "The current learning rate: 0.00047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17000/20000 [50:20<04:36, 10.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17000: train loss 2.5357, val loss 2.5398\n",
      "The current learning rate: 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 17499/20000 [51:50<03:51, 10.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17500: train loss 2.5043, val loss 2.5090\n",
      "The current learning rate: 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 17999/20000 [53:20<03:12, 10.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18000: train loss 2.4714, val loss 2.4754\n",
      "The current learning rate: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 18499/20000 [54:50<02:21, 10.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18500: train loss 2.4445, val loss 2.4456\n",
      "The current learning rate: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19000/20000 [56:17<01:55,  8.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19000: train loss 2.4274, val loss 2.4211\n",
      "The current learning rate: 0.00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 19500/20000 [58:10<00:47, 10.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19500: train loss 2.3900, val loss 2.3957\n",
      "The current learning rate: 0.00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [59:29<00:00,  5.60it/s]  \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e827b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adf1b9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = GPT(config)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb3c7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. A bee was tied to the memory of the flames. The sky was red and too shiny in their nature. The eagle could painted a beautiful picture of a rainbow.\n",
      "\n",
      "Every day the plant was so happy and sticky. It had a secret to its kingdom. The rabbit went back to playing in the sky. The Pick this day, and and the seed became the bitter. The bird was very sad, but the giant brave bird knew that the light could spark walking by. So, the bean decided that so it could fly away from soon. The cloud decided to go home with its brush.\n",
      "\n",
      "So the stone around the butterfly's wing, the bird found a step and got all into all the well. The creature part was so tall that theBaby curly white caterpillar had made the grandmother feel carefully, so proud and loved that she was always able to pick it bigger and strong in the sky. It was star day too than to fit with the night white flowers were spark and it\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d03a5827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A little girl went to the woods. There were lots of darkness and they had lots of flowers and birds it a elephants. They looked like yummy! Each child kept looking for anything they could eat.\n",
      "\n",
      "He wasbe about to come by the fields and hoping to find a message in it - looking for someone who looked under and questions. That will She decided to enter the forest.\n",
      "\n",
      "So,, he had an idea. He said quickly and ran with a shiny bow on his tear. He jumped onto the light and escaped. She felt like he was in the happening and meeting it to him.\n",
      "\n",
      "At the end of the way, he knew he could do something. He took out his turn and to the singing. He watched as he said he was ready, but nothing seemed. He had to promise to himself.\n",
      "\n",
      "The end.Once upon a time there was a good boy. It was very careful rebuild its stand, but not clean, her work made sure to only. Jelly...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A little girl went to the woods\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74003c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
